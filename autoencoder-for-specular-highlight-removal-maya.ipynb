{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"> ## Introduction:\n\nThe code below is adapted from the notebook that uses Autoencoder as Feature Extractor - CIFAR10\n\nI used both general convolutional autoencoder and [U-net](https://arxiv.org/pdf/1505.04597.pdf) model as autoencoder.\n\nThe implementation for U-net model is taken from [here](https://towardsdatascience.com/understanding-semantic-segmentation-with-unet-6be4f42d4b47)\n\nThe implementation for U-net loss function is taken from [here](https://github.com/shibuiwilliam/Keras_Autoencoder/blob/master/Cifar_Conv_AutoEncoder_UNET.ipynb)","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport pickle\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport sys\n\nimport tensorflow as tf\nfrom PIL import Image\nimport skimage\nfrom skimage.viewer import ImageViewer\n\nimport keras\nfrom keras.layers import *\nfrom keras import regularizers\nfrom keras.models import Model, Sequential\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nfrom keras.optimizers import SGD, Adam, RMSprop, Adadelta\nimport keras.backend as K\nfrom keras.losses import mean_squared_error\nfrom keras.preprocessing.image import ImageDataGenerator,array_to_img, img_to_array, load_img\nfrom keras.utils import np_utils\nfrom keras.applications.vgg16 import VGG16\n\n\nfrom sklearn.utils import class_weight\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler, LabelBinarizer, RobustScaler, StandardScaler\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ## Image Data Generator","metadata":{}},{"cell_type":"raw","source":"import os\nos.listdir('/kaggle/input/d/ooiyueying/real-specular/real_specular/')","metadata":{"trusted":true}},{"cell_type":"code","source":"image_shape = 64\nfname = \"/kaggle/input/mayadatasetv2/input/train/train_x/x/Female_1.01_scene2.jpg\"\nimg = load_img(fname,target_size=(image_shape,image_shape))\nplt.figure(figsize=(20, 4))\nplt.imshow(img)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_float = np.array(img).astype('float32')\nx = x_float/255\nx = x.reshape((image_shape,image_shape,3))\ncropped=x[5:37,16:48,]\nprint(cropped.shape)\nplt.figure(figsize=(20, 4))\nplt.imshow(cropped)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_model_WH = 256\ninput_model_shape = (256,256,3)\ninput_image_WH = 512\ninput_image_shape = (512,512,3)\nbatch_size = 32\n\nBASE_DIR = \"/kaggle/input/mayadatasetv2/input/\"\n# train_x_dir = BASE_DIR + \"train/train_x/\"\n# train_y_dir= BASE_DIR + \"train/train_y/\"\ntrain_x_dir = BASE_DIR + \"train_with_identity/train_with_identity_x/\"\ntrain_y_dir= BASE_DIR + \"train_with_identity/train_with_identity_y/\"\nval_x_dir = BASE_DIR + \"val/val_x/\"\nval_y_dir = BASE_DIR + \"val/val_y/\"\n# test_x_dir = BASE_DIR + \"test/test_x/\"\n# test_y_dir = BASE_DIR + \"/test/test_y/\"\n\n\ndata_flow_args = dict(\n    target_size=(input_image_WH,input_image_WH),\n    batch_size=batch_size,\n    classes=None, \n    class_mode=None,\n    shuffle=False,\n    color_mode='rgb')\n\n# A separate dataflow configuration, where batch_size= 1, so that predict generator is in order\ntest_data_flow_args = dict(\n    target_size=(input_image_WH,input_image_WH),\n    batch_size=600,\n    classes=None, \n    class_mode=None,\n    shuffle=False,\n    color_mode='rgb')\n\ndef im_crop(image):\n    im_cropped = image[30:286,130:386,]\n    return im_cropped\n\n\ndatagenerator = ImageDataGenerator(rescale=1./255)\n\n# train_x_batches = datagenerator.flow_from_directory('/kaggle/input/mayadatasetv2/input/train/train_x/',\n#     **data_flow_args)\n# train_y_batches = datagenerator.flow_from_directory('/kaggle/input/mayadatasetv2/input/train/train_y/',\n#     **data_flow_args)\ntrain_x_batches = datagenerator.flow_from_directory('/kaggle/input/mayadatasetv2/input/train_with_identity/train_with_identity_x/',\n    **data_flow_args)\ntrain_y_batches = datagenerator.flow_from_directory('/kaggle/input/mayadatasetv2/input/train_with_identity/train_with_identity_y/',\n    **data_flow_args)\nval_x_batches = datagenerator.flow_from_directory('/kaggle/input/mayadatasetv2/input/val/val_x/',\n    **data_flow_args)\nval_y_batches = datagenerator.flow_from_directory('/kaggle/input/mayadatasetv2/input/val/val_y/',\n    **data_flow_args)\n# test_x_batches = datagenerator.flow_from_directory('/kaggle/input/mayadatasetv2/input/test/test_x/',\n#     **test_data_flow_args)\n# test_y_batches = datagenerator.flow_from_directory('/kaggle/input/mayadatasetv2/input/test/test_y/',\n#     **test_data_flow_args)\n\ndef zip_generator(x, y):\n    while True:\n        batch_x = next(x)\n        batch_y = next(y)\n        batch_x_crops = np.zeros((batch_x.shape[0], input_model_WH, input_model_WH, 3))\n        batch_y_crops = np.zeros((batch_y.shape[0], input_model_WH, input_model_WH, 3))\n        # go through each of the images\n        for i in range(batch_x.shape[0]):\n            batch_x_crops[i] = im_crop(batch_x[i])\n            batch_y_crops[i] = im_crop(batch_y[i])\n        yield batch_x_crops, batch_y_crops\n        \ntrain_batches = zip_generator(train_x_batches, train_y_batches)\nval_batches = zip_generator(val_x_batches, val_y_batches)\n# test_batches = zip_generator(test_x_batches, test_y_batches)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Utility functions:","metadata":{}},{"cell_type":"code","source":"def create_block(input, chs): ## Convolution block of 2 layers for conv autoencoder\n    x = input\n    for i in range(2):\n        x = Conv2D(chs, 3, padding=\"same\")(x)\n        x = Activation(\"relu\")(x)\n        x = BatchNormalization()(x)\n    return x\n\ndef conv2d_block(input_tensor, n_filters, kernel_size = 3, batchnorm = True):  ## Convolution block of 2 layers for unet autoencoder\n    \"\"\"Function to add 2 convolutional layers with the parameters passed to it\"\"\"\n    # first layer\n    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),\\\n              kernel_initializer = 'he_normal', padding = 'same')(input_tensor)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    \n    # second layer\n    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),\\\n              kernel_initializer = 'he_normal', padding = 'same')(input_tensor)\n    if batchnorm:\n        x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    \n    return x\n\ndef showOrigDec(orig, dec,num=10):  ## function used for visualizing original and reconstructed images of the autoencoder model\n    \n    plt.figure(figsize=(20, 4))\n    \n    for i in range(num):\n        # display original\n        ax = plt.subplot(2, num, i+1)\n        plt.imshow(orig[i])\n        ax.get_xaxis().set_visible(False)\n        ax.get_yaxis().set_visible(False)\n        \n        # display reconstruction\n        ax = plt.subplot(2, num, i +1+num)\n        plt.imshow(dec[i])\n        ax.get_xaxis().set_visible(False)\n        ax.get_yaxis().set_visible(False)\n        \n    plt.show()\n        \n    \ndef pixel_loss(y_true, y_pred):  ## loss function for using in conv autoencoder models\n    mses = mean_squared_error(y_true, y_pred)\n    return K.sum(mses, axis=(1,2))\n\n# comparing in feature space\ndef perceptual_loss(y_true, y_pred):\n    vgg = VGG16(include_top=False, weights='imagenet', input_shape=(input_model_WH,input_model_WH,3))\n    loss_model = Model(inputs=vgg.input, outputs=vgg.get_layer('block3_conv3').output)\n    loss_model.trainable = False\n    return K.mean(K.square(loss_model(y_true) - loss_model(y_pred)))\n\n\n# TODO: compare in pixel level + perceptual_loss\ndef loss_function(y_true, y_pred):\n    return 0.2*pixel_loss(y_true, y_pred)+0.8*perceptual_loss(y_true, y_pred)\n\n'''\nreplace conv2DTranspose \n''' \ndef create_upsample(ngf, mult, input_tensor):\n    x = UpSampling2D((2, 2), interpolation='bilinear')(input_tensor)\n    x = Conv2D(ngf * mult, (3, 3), strides=(1,1), activation='relu', kernel_initializer='he_normal', padding='same')(x)\n    return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x, y = next(train_batches)\n\nshowOrigDec(x,y,0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Autoencoder Models(Unet and General Convolutional AE):","metadata":{}},{"cell_type":"code","source":"def get_unet(n_filters = 16, dropout = 0.1, batchnorm = True):\n    input_img = Input(input_model_shape)\n    # Contracting Path\n    c1 = conv2d_block(input_img, n_filters * 1, kernel_size = 3, batchnorm = batchnorm)\n    p1 = MaxPooling2D((2, 2))(c1)\n    p1 = Dropout(dropout)(p1)\n\n    c2 = conv2d_block(p1, n_filters * 2, kernel_size = 3, batchnorm = batchnorm)\n    p2 = MaxPooling2D((2, 2))(c2)\n    p2 = Dropout(dropout)(p2)\n\n    c3 = conv2d_block(p2, n_filters * 4, kernel_size = 3, batchnorm = batchnorm)\n    p3 = MaxPooling2D((2, 2))(c3)\n    p3 = Dropout(dropout)(p3)\n\n    c4 = conv2d_block(p3, n_filters * 8, kernel_size = 3, batchnorm = batchnorm)\n    p4 = MaxPooling2D((2, 2))(c4)\n    p4 = Dropout(dropout)(p4)\n\n    c5 = conv2d_block(p4, n_filters = n_filters * 16, kernel_size = 3, batchnorm = batchnorm)\n\n    # Expansive Path\n#     u6 = Conv2DTranspose(n_filters * 8, (3, 3), strides = (2, 2), padding = 'same')(c5)\n    u6 = create_upsample(n_filters, 8, c5)\n    u6 = concatenate([u6, c4])\n    u6 = Dropout(dropout)(u6)\n    c6 = conv2d_block(u6, n_filters * 8, kernel_size = 3, batchnorm = batchnorm)\n\n#     u7 = Conv2DTranspose(n_filters * 4, (3, 3), strides = (2, 2), padding = 'same')(c6)\n    u7 = create_upsample(n_filters, 4, c6)\n    u7 = concatenate([u7, c3])\n    u7 = Dropout(dropout)(u7)\n    c7 = conv2d_block(u7, n_filters * 4, kernel_size = 3, batchnorm = batchnorm)\n\n#     u8 = Conv2DTranspose(n_filters * 2, (3, 3), strides = (2, 2), padding = 'same')(c7)\n    u8 = create_upsample(n_filters, 2, c7)\n    u8 = concatenate([u8, c2])\n    u8 = Dropout(dropout)(u8)\n    c8 = conv2d_block(u8, n_filters * 2, kernel_size = 3, batchnorm = batchnorm)\n\n#     u9 = Conv2DTranspose(n_filters * 1, (3, 3), strides = (2, 2), padding = 'same')(c8)\n    u9 = create_upsample(n_filters,1, c8)\n    u9 = concatenate([u9, c1])\n    u9 = Dropout(dropout)(u9)\n    c9 = conv2d_block(u9, n_filters * 1, kernel_size = 3, batchnorm = batchnorm)\n\n    outputs = Conv2D(3, (1, 1), activation='sigmoid')(c9)\n    model = Model(inputs=[input_img], outputs=[outputs])\n    return model\n\n\ndef get_conv():\n    \n    input = Input(input_model_shape)\n    \n    # Encoder\n    block1 = create_block(input, 32)\n    x = MaxPool2D(2)(block1)\n    block2 = create_block(x, 64)\n    x = MaxPool2D(2)(block2)\n    \n    \n    #Middle\n    middle = create_block(x, 128)\n#     middle = AdaIN()(encoder.outputs)\n    \n    # Decoder\n    up1 = UpSampling2D((2,2))(middle)\n    block3 = create_block(up1, 64)\n    #up1 = UpSampling2D((2,2))(block3)\n    up2 = UpSampling2D((2,2))(block3)\n    block4 = create_block(up2, 32)\n    #up2 = UpSampling2D((2,2))(block4)\n    \n    # output\n    x = Conv2D(3, 1)(up2)\n    output = Activation(\"sigmoid\")(x)\n    \n    return Model(input, output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_ae(m):  ## function for choosing unet/general conv autoencoder\n    if m=='unet':\n        return get_unet()\n    elif m=='ae':\n        model = get_conv()\n        return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run U-Net:","metadata":{}},{"cell_type":"code","source":"model_unet = run_ae('unet')\nmodel_unet.compile(optimizer='adam', loss=loss_function)\nmodel_unet.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"er = EarlyStopping(monitor='val_loss', patience=50, mode='min',restore_best_weights=True)\nlr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10)\ncallbacks = [er, lr]\n# history = model_unet.fit(x_train, y_train, \n#                          batch_size=512,\n#                          epochs=100,\n#                          verbose=1,\n#                          validation_data=(x_val, y_val),\n#                          shuffle=True, callbacks=callbacks)\n\nhistory = model_unet.fit_generator(train_batches,\n                    steps_per_epoch = train_x_batches.samples // batch_size,\n                    epochs=200,\n                    verbose=1, \n                    validation_data=val_batches,\n                    validation_steps = val_x_batches.samples // batch_size,\n#                     callbacks=callbacks,\n                    use_multiprocessing=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Val'], loc='lower right')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\n\nimage_shape = 512\ndef load_reshape_img(fname, image_shape=512):\n    img = load_img(fname,target_size=(image_shape,image_shape))\n    x_float = np.array(img).astype('float32')\n    x = x_float/255\n    x = x.reshape((image_shape,image_shape,3))\n    return x\n\ndef generate_test():\n    MAIN_x_DIR = \"../input/mayadatasetv2/input/test/test_x/x/\"\n    MAIN_y_DIR = \"../input/mayadatasetv2/input/test/test_y/y/\"\n    image_x_df = []\n    image_y_df = []\n    female_idx = [18, 18, 21, 22, 19, 22, 21, 19, 21, 20]\n    angle_idx = [6, 8, 6, 2, 8, 10, 8, 8, 11, 6]\n    scene = [4, 6, 6, 7, 2, 2, 1, 7, 9, 1]\n    for idx in range(1,11):\n        # ../input/mayadatasetv2/input/test/test_x/x/Female_18.05_scene9.jpg\n        # ../input/mayadatasetv2/input/test/test_x/x/Female_18.04_scene9.jpg\n        # ../input/mayadatasetv2/input/test/test_x/x/Female_18.04_scene1.jpg\n        IMAGE_PATH = \"Female_\"+str(female_idx[idx-1])+\".\"+'{:02}'.format(angle_idx[idx-1])+\"_scene\"+str(scene[idx-1])+\".jpg\"\n        im_x = load_reshape_img(MAIN_x_DIR+IMAGE_PATH)\n        im_y = load_reshape_img(MAIN_y_DIR+IMAGE_PATH)\n        image_x_df.append(im_x[30:286,130:386,])\n        image_y_df.append(im_y[30:286,130:386,])\n    return np.array(image_x_df),np.array(image_y_df)\n\n# def generate_real_test():\n#     #../input/real-specular/real_specular/Image-12.jpg\n#     MAIN_DIR = \"/kaggle/input/d/ooiyueying/real-specular/real_specular/\"\n#     image_df = []\n#     for i in range(12,18):\n#         print(MAIN_DIR+\"Image-\"+str(i)+\".jpg\")\n#         im = load_reshape_img( MAIN_DIR+\"Image-\"+str(i)+\".jpg\", 256)\n#         image_df.append(im)\n#     return np.array(image_df)\n\ndef generate_real_test():\n    #../input/real-specular/real_specular/Image-12.jpg\n    MAIN_DIR = \"../input/realspecularimages/real_specular_images/\"\n    image_df = []\n    for i in range(8):\n        print(MAIN_DIR+\"Capture\"+str(i)+\".jpg\")\n        im = load_reshape_img( MAIN_DIR+\"Capture\"+str(i)+\".JPG\", 256)\n        image_df.append(im)\n    return np.array(image_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.listdir(\"../input/realspecularimages/real_specular_images/\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = generate_real_test()\n\n\nrecon_test_unet = model_unet.predict(x)\n\nshowOrigDec(x, recon_test_unet, 6)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x, y = generate_test()\n\nshowOrigDec(x, y, 10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"recon_test_unet = model_unet.predict(x)\n\nshowOrigDec(x, recon_test_unet, 10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x, y = generate_test()\n\nshowOrigDec(x, y, 10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# recon_test_unet = model_unet.predict(x)\n\n# showOrigDec(x, recon_test_unet, 10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# x, y = generate_test()\n\n# showOrigDec(x, y, 10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# recon_test_unet = model_unet.predict(x)\n\n# showOrigDec(x, recon_test_unet, 10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run Convolutional AE:","metadata":{}},{"cell_type":"code","source":"# model_ae = run_ae('ae')\n# model_ae.compile(\"adam\", loss=loss_function)\n# model_ae.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# er = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n# lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_delta=0.0001)\n# callbacks = [er, lr]\n# history = model_ae.fit(x_train, y_train, \n#                        batch_size=128,\n#                        epochs=100,\n#                        verbose=1,\n#                        validation_data=(x_val, y_val),\n#                        shuffle=True, callbacks=callbacks)\n\n# history = model_ae.fit_generator(train_batches,\n#                     steps_per_epoch = train_x_batches.samples // batch_size,\n#                     epochs=70,\n#                     verbose=1, \n#                     validation_data=val_batches,\n#                     validation_steps = val_x_batches.samples // batch_size,\n# #                     callbacks=callbacks,\n#                     use_multiprocessing=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.plot(history.history['loss'])\n# plt.plot(history.history['val_loss'])\n# plt.title('Model Loss')\n# plt.ylabel('Loss')\n# plt.xlabel('Epoch')\n# plt.legend(['Train', 'Val'], loc='lower right')\n# plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# recon_test_ae = model_ae.predict_generator(test_batches, 10)\n# recon_valid_ae = model_ae.predict_generator(val_batches, 10)\n# recon_train_ae = model_ae.predict_generator(train_batches, 10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# showOrigDec(x, y, 10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# recon_test_unet = model_ae.predict(x)\n\n# showOrigDec(x, recon_test_unet, 10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}